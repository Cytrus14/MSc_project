{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d63906b-83d3-40ed-8a40-cc92fe427500",
   "metadata": {},
   "source": [
    "# Benchmark config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4a3829-e8bb-469b-9057-dd212590b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_enabled = True\n",
    "quantization = '8bit' # Valid values: None, '8bit', '4bit'\n",
    "results_file_name = 'rag_8bit.csv'\n",
    "\n",
    "RAGPipeline_module_dir = '/home/matlab/data/Dominik/app'\n",
    "rag_adapter_path = './fine_tuning/fine_tuned_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e504e6-c31e-49f7-bef3-7fda89f0d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import MistralConfig, BitsAndBytesConfig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if rag_enabled:\n",
    "    sys.path.append(RAGPipeline_module_dir)\n",
    "    from RAGPipeline import RAGPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5d0be5-c554-4862-8633-6a3614379544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample_questions(sample_questions):\n",
    "    letter_answers = ['A', 'B', 'C', 'D']\n",
    "    sample_questions_dicts = []\n",
    "    for idx in range(len(sample_questions['question'])):\n",
    "        sample_question_dict = {}\n",
    "        sample_question_dict['question'] = sample_questions['question'][idx]\n",
    "        sample_question_dict['answer_A'] = sample_questions['choices'][idx][0]\n",
    "        sample_question_dict['answer_B'] = sample_questions['choices'][idx][1]\n",
    "        sample_question_dict['answer_C'] = sample_questions['choices'][idx][2]\n",
    "        sample_question_dict['answer_D'] = sample_questions['choices'][idx][3]\n",
    "        sample_question_dict['correct_answer'] = letter_answers[sample_questions['answer'][idx]]\n",
    "        sample_questions_dicts.append(sample_question_dict)\n",
    "    return sample_questions_dicts\n",
    "\n",
    "\n",
    "def prepare_prompt(domain, sample_questions, question, question_context = '\\nNone\\n'):\n",
    "    domain = re.sub(\"_\", \" \", domain)\n",
    "    # Prompt begining\n",
    "    prompt = \"<s>[INST] The following are multiple choice questions (with answers) about {} with optional, helpful context for the last question.\".format(domain)\n",
    "    prompt += \"\\n\\n<context>{}</context>\\n\".format(question_context)\n",
    "    question_template = \"\\nQuestion: {}\\nA: {}\\nB: {}\\nC: {}\\nD: {}\\nAnswer: {}\"\n",
    "    # Sample questions with answers\n",
    "    for example in sample_questions:\n",
    "        example_question = question_template.format(\n",
    "            example['question'],\n",
    "            example['answer_A'],\n",
    "            example['answer_B'],\n",
    "            example['answer_C'],\n",
    "            example['answer_D'],\n",
    "            example['correct_answer']\n",
    "        )\n",
    "        prompt += example_question + '\\n'\n",
    "    # Question that the model must answer\n",
    "    question = question_template.format(\n",
    "            question['question'],\n",
    "            question['choices'][0],\n",
    "            question['choices'][1],\n",
    "            question['choices'][2],\n",
    "            question['choices'][3],\n",
    "            ''\n",
    "        )\n",
    "    prompt += question + '[/INST]'\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def convert_answer(answer):\n",
    "    \"\"\"\n",
    "    Convert the model's answer (A, B, C or D) to a numerical equivalent in the MMLU\n",
    "    benchmark\n",
    "\n",
    "    Parameters:\n",
    "    answer (str): the model's answer (only the first character matters)\n",
    "\n",
    "    Returns:\n",
    "    (int): numerical equivalent to the answer in the MMLU dataset\n",
    "    \"\"\"\n",
    "    if len(answer) == 0:\n",
    "        return 4 # No answer doesn't meet requirements\n",
    "    elif answer[0] == 'A' or answer[0] == 'a':\n",
    "        return 0\n",
    "    elif answer[0] == 'B' or answer[0] == 'b':\n",
    "        return 1\n",
    "    elif answer[0] == 'C' or answer[0] == 'c':\n",
    "        return 2\n",
    "    elif answer[0] == 'D' or answer[0] == 'd':\n",
    "        return 3\n",
    "    else:\n",
    "        return 4 # Answer doesn't meet stated requirements\n",
    "\n",
    "def sanitize_IDs(model_output, ids_max_count=4, max_id=7):\n",
    "    sanitized_IDs = []\n",
    "    # If the model didn't output anything, return an empty list\n",
    "    if len(model_output) == 0:\n",
    "        return []\n",
    "    for idx, el in enumerate(model_output):\n",
    "        # print(el[0])\n",
    "        # Break the loop after reaching idx_max_count of if el\n",
    "        # is an empty string\n",
    "        if idx == ids_max_count or el == '':\n",
    "            break\n",
    "        # If there's a -1, then non of the ids are relevant - return an empty list\n",
    "        elif el[0].isdigit() and int(el[0]) == -1:\n",
    "            return []\n",
    "        elif el[0].isdigit() and int(el[0]) >= 0 and int(el[0]) <= max_id:\n",
    "            sanitized_IDs.append(int(el[0]))\n",
    "        else:\n",
    "            break\n",
    "    return sanitized_IDs\n",
    "\n",
    "def select_docs_by_id(documents_raw, ids):\n",
    "    documents_selected = []\n",
    "    for doc in documents_raw:\n",
    "        if doc[0].metadata['ID'] in ids:\n",
    "            documents_selected.append(doc)\n",
    "    return documents_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33970717-4ce9-458a-8892-c760edb593aa",
   "metadata": {},
   "source": [
    "# Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5e65d5-f1f5-4419-83f4-c72dc55aa30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matlab/miniconda3/envs/llm_student_msc/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c9b6e22487482894e12f9cb56b075a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 8-bit quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline loaded\n"
     ]
    }
   ],
   "source": [
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Choose quantization type\n",
    "if quantization == '8bit':\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model, config=MistralConfig, quantization_config=bnb_config, device_map='cuda')\n",
    "    print('Model loaded with 8-bit quantization')\n",
    "elif quantization == '4bit':\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model, config=MistralConfig, quantization_config=bnb_config, device_map='cuda')\n",
    "    print('Model loaded with 4-bit quantization')\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model, config=MistralConfig, device_map='cuda')\n",
    "    print('Model loaded with no quantization')\n",
    "\n",
    "# Load the RAG adapter if RAG is enabled\n",
    "if rag_enabled:\n",
    "    model.load_adapter(rag_adapter_path, adapter_name='rag_adapter')\n",
    "    model.set_adapter('rag_adapter')\n",
    "    model.disable_adapters()\n",
    "    rag_pipeline = RAGPipeline()\n",
    "    print('RAG pipeline loaded')\n",
    "    rag_prompt_template = \"\"\"<s>[INST] Below is a list of documents. Return up to 4 IDs of documents most useful for solving the user_prompt. If no documents are relevant, output -1. {format}. \n",
    "    \n",
    "<documents>\n",
    "{documents}\n",
    "</documents>\n",
    "\n",
    "user_prompt: {user_prompt}\n",
    "\n",
    "[/INST]IDs: \"\"\"\n",
    "    output_parser = CommaSeparatedListOutputParser()\n",
    "else:\n",
    "    print('RAG disabled')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", config=MistralConfig)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1, device=0)\n",
    "# pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "# LLM = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# prompt_template = \"\"\"<s>[INST] Your objective is to select the right answer for the question stated below.\n",
    "# If question 'A' is the correct answer, output 'A'; if 'B' is the correct answer, output 'B', and so on.\n",
    "# Do not explain or justify your answer. Your output must be a single letter: 'A', 'B', 'C', or 'D'.\n",
    "# Question: {question}\n",
    "# A: {answer_A}\n",
    "# B: {answer_B}\n",
    "# C: {answer_C}\n",
    "# D: {answer_D}\n",
    "# Your answer: [/INST]\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate.from_template(prompt_template)\n",
    "# chain = prompt | LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6120d49b-c5cd-4d0d-be09-f97e5057cd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7826a446e0f48e9a31a0574eb60bfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4201a8a8cb3d443ca679e3a4f1d7c53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6850f27f8d46c4a4822a6fe78035b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93830e5715bf4ac2a567204c38a0d60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75164eb2361446c8b641869a585c187c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab87c6e3887e43e3b2309c031a00cabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5357ad2697434d94611d9b90fb6592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b33b33765a49a5a60b47f472e76558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e532cfd4d04e0f8173ec7689c4a35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36afbaa80fc48b0ad6066ef00f466b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6225ca2b8e6744c2a89cc3de3af71674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4082d813d4ae47bca20d6ecf231d8c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cf40f0ca5c47f295d7fd13af74d977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c7094f5c5b41148576ee768584764a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9e5a6c99e1404ea1798b585f5475a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a83e1915074ef9a68566b4b862c793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cbb0d899a44e83852883a8637089e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5a994a334e4af6ba9e0b7f76f8a8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec0dac8b4f140e4b11f025f5f987f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ad0680be684910ae10cdb42358077b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52368ffbeb2f43a68ff28c0979e1ea99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de15071e4a7c461d9a7cda70973c32bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1480464c964542b1f45bb23f1d4b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643eb83f61fd4bce9f7ac78ee4684697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed226d9c4fd4a949bb15696e6110451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f680aa05291449a2a7e442df0b285277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e02ad56f81456eb6025ee991003cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7057eea9ba0243168408b58374b9211d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cbf181f14a4283b232b32f868327f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabe35fcf5404d91afd1fa1c2262a9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d501d360de491ba1854379cf7c20f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560cdd5b001b42978bd8b18e5ad24648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf732288a6b47e6b10236e73b5e97a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897f72629a2a4df59c275245c58d26da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7b41735b1d4c84a246d969a7a74ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47509be69a4347868cfa6a9f3ac8ba36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7a222b5b6f4635908d931290276e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653df8d7385a4ad99531abf17a35cc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ad3ea6482c4842a7935862d40986a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd401e27fc44e99af283a6ee4a02d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb194028ea64898b78c2a086ef54659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ff1c95fceb470f89de782d4a57a7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9f56f6945942459c475e272653f31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3503f453accc4797b6b47eeb791a3e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e653cc2c74497db7a5bfd52920ff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea1945143534e039ae5388b4270d34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2782f3f304f4f1fbe75391504aeffc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00376788c9242108ac2b1faf3aa3140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6fc01f392a439b84c55abf8505ede9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b0cd40bc854d46ba2577b8fd30d2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a742f5d5d3c14f8caca61cb86a9c5b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a020852e0243059f81d2bbd799bd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc94134be75249689cace58ae4838e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5b65519d1c4177aab67f7f46d793b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b4e459087144f9bb2451ba09cb3716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c444d9cef7264fee9d6c2d6333d976c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3669211d3c84ab3a05fe7f5046f895a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MMLU_domains = ['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science',\n",
    "                'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering',\n",
    "                'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science',\n",
    "                'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics',\n",
    "                'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history',\n",
    "                'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics',\n",
    "                'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law',\n",
    "                'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n",
    "right_answers = {}\n",
    "question_count = {}\n",
    "bad_format_answers = {}\n",
    "for domain in MMLU_domains:\n",
    "    samples_changed = False\n",
    "    dataset = load_dataset(\"cais/mmlu\", domain, split='test', trust_remote_code=True)\n",
    "    right_answers[domain] = 0\n",
    "    bad_format_answers[domain] = 0\n",
    "    question_count[domain] = len(dataset)\n",
    "    # Prepare sample questions\n",
    "    sample_questions = dataset[-5:]\n",
    "    sample_questions = prepare_sample_questions(sample_questions)\n",
    "    # Prepare sample questions\n",
    "    for idx, question in enumerate(tqdm(dataset)):\n",
    "        ### Inference goes here\n",
    "        if rag_enabled:\n",
    "            torch.cuda.empty_cache()\n",
    "            prompt = question['question']\n",
    "            documents_raw = rag_pipeline.retrieve_relevant_data(prompt)\n",
    "            if len(documents_raw) > 0:\n",
    "                documents_llm = rag_pipeline.format_docs_for_LLM(documents_raw)\n",
    "                RAG_prompt = rag_prompt_template.format(format=output_parser.get_format_instructions(), documents=documents_llm, user_prompt=prompt)\n",
    "                tokenized_context = tokenizer(RAG_prompt, return_tensors=\"pt\").to('cuda')\n",
    "                # Bypass the LLM filter if there are too many tokens to handle\n",
    "                if len(tokenized_context.input_ids[0]) > 5000:\n",
    "                    if len(documents_raw) > 3:\n",
    "                        document_IDs_sanitized = [0,1,2,3]\n",
    "                    else:\n",
    "                        document_IDs_sanitized = [i for i in range(len(documents_raw))]\n",
    "                else:\n",
    "                    model.enable_adapters()\n",
    "                    response = model.generate(tokenized_context.input_ids, pad_token_id=2, attention_mask=tokenized_context.attention_mask, do_sample=False, max_new_tokens=8)\n",
    "                    model.disable_adapters()\n",
    "                    response = response[0][tokenized_context.input_ids.shape[1]:] # Remove the input from the output\n",
    "                    output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "                    document_IDs = output_parser.parse(output)\n",
    "                    document_IDs_sanitized = sanitize_IDs(document_IDs)\n",
    "                if len(document_IDs_sanitized) > 0:\n",
    "                    relevant_documents_raw = select_docs_by_id(documents_raw, document_IDs_sanitized)\n",
    "                    relevant_documents_llm = rag_pipeline.format_doc_for_LLM_no_ids(relevant_documents_raw)\n",
    "                    relevant_documents_llm = \"\\n\" + relevant_documents_llm\n",
    "                if len(relevant_documents_raw) > 0:\n",
    "                    prompt = prepare_prompt(domain, sample_questions, question, relevant_documents_llm)\n",
    "                else:\n",
    "                    prompt = prepare_prompt(domain, sample_questions, question)\n",
    "        else:\n",
    "            prompt = prepare_prompt(domain, sample_questions, question)\n",
    "        tokenized_context = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        response = model.generate(tokenized_context.input_ids, pad_token_id=2, attention_mask=tokenized_context.attention_mask, do_sample=False, max_new_tokens=1)\n",
    "        response = response[0][tokenized_context.input_ids.shape[1]:] # Remove the input from the output\n",
    "        answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        ### Inference goes here\n",
    "        llm_answer = convert_answer(answer)\n",
    "        if llm_answer == question['answer']:\n",
    "            right_answers[domain] += 1\n",
    "        elif llm_answer == 4:\n",
    "            bad_format_answers[domain] += 1\n",
    "        # After going over approx 50% of questions, select sample questions from\n",
    "        # beginning of the dataset\n",
    "        if not samples_changed and idx > len(dataset) / 2:\n",
    "            sample_questions = dataset[:5]\n",
    "            sample_questions = prepare_sample_questions(sample_questions)\n",
    "            samples_changed = True\n",
    "            torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfea54c-de01-4e21-9078-732a9d8be7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\n",
    "    'category',\n",
    "    'question_count',\n",
    "    'right_answers',\n",
    "    'right_answers_percent',\n",
    "    'right_answers_percent_(ignore_bad_format)',\n",
    "    'bad_format_answers',\n",
    "    'bad_format_answers_percent'\n",
    "])\n",
    "for domain in MMLU_domains:\n",
    "    right_answers_percent = round(right_answers[domain] / question_count[domain] * 100, 3)\n",
    "    right_answers_percent_ignore_bad_format = round(right_answers[domain] / (question_count[domain] - bad_format_answers[domain]) * 100, 3)\n",
    "    bad_format_answers_percent = round(bad_format_answers[domain] / question_count[domain] * 100, 3)\n",
    "    row = {\n",
    "        'category': domain, \n",
    "        'question_count': question_count[domain],\n",
    "        'right_answers': right_answers[domain],\n",
    "        'right_answers_percent': right_answers_percent,\n",
    "        'right_answers_percent_(ignore_bad_format)': right_answers_percent_ignore_bad_format,\n",
    "        'bad_format_answers': bad_format_answers[domain],\n",
    "        'bad_format_answers_percent': bad_format_answers_percent\n",
    "    }\n",
    "    results.loc[len(results)] = row\n",
    "results.to_csv(results_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb41e5-f015-47d8-a141-d934378fb298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
