{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4539f9-6b9c-4aa7-94a2-b2bf4434f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matlab/miniconda3/envs/llm_student_msc/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/matlab/miniconda3/envs/llm_student_msc/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/matlab/miniconda3/envs/llm_student_msc/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import MistralConfig\n",
    "\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import chromadb\n",
    "import nltk\n",
    "import spacy\n",
    "from datasets import Dataset\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6bf250-c06d-43b6-983e-518068686c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"text\":[]}\n",
    "\n",
    "def save_json(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, ensure_ascii=False)\n",
    "\n",
    "def extract_keywords(string):\n",
    "    # Extract keywords from the prompt\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(string)\n",
    "    keywords = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if not chunk.text.lower().strip() in nltk.corpus.stopwords.words('english'):\n",
    "            text = chunk.text\n",
    "            # Remove indirect articles\n",
    "            text = text.replace('a ', '').replace('an ', '').strip()\n",
    "            keywords.append(text)\n",
    "    # Convert keywords to their singular forms\n",
    "    keywords_singular = [lemmatizer.lemmatize(word) for word in keywords]\n",
    "    return keywords_singular\n",
    "\n",
    "def contains_keywords_filter(keywords, docs):\n",
    "    # Filter data by keywords\n",
    "    filtered_data = []\n",
    "    if len(keywords) > 0:\n",
    "        for doc in docs:\n",
    "            el = doc[0].page_content\n",
    "            if any(keyword in el for keyword in keywords):\n",
    "                filtered_data.append(doc)\n",
    "        return filtered_data\n",
    "    else:\n",
    "        return docs\n",
    "\n",
    "# def format_docs_for_LLM(docs):\n",
    "#     documents = []\n",
    "#     for doc in docs:\n",
    "#         documents.append(doc[0].page_content.replace(\"search_document: \", '', 1))\n",
    "#     return documents\n",
    "\n",
    "def format_docs_for_LLM(docs):\n",
    "    formated_documents = \"\"\n",
    "    for idx, doc in enumerate(docs):\n",
    "        page_content = \"ID {}:\\n\".format(idx)\n",
    "        page_content += \"Title: {}\\n\".format(doc[0].metadata['title'])\n",
    "        page_content += doc[0].page_content.replace(\"search_document: \", '', 1)\n",
    "        page_content += \"\\n\\n\"\n",
    "        formated_documents += page_content\n",
    "    return formated_documents\n",
    "\n",
    "\n",
    "def extract_titles_from_docs(docs):\n",
    "    titles = set()\n",
    "    for idx, doc in enumerate(docs):\n",
    "        titles.add(doc[0].metadata['title'])\n",
    "    return titles\n",
    "\n",
    "file_path = os.path.join('data', 'fine_tuning', 'data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18dfa88-8fbd-4141-b329-de987c091010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.3.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/home/matlab/miniconda3/envs/llm_student_msc/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs={\n",
    "        'device': 'cuda',\n",
    "        'trust_remote_code': True\n",
    "    }\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path='chroma_data')\n",
    "langchain_vector_db = Chroma(client=chroma_client, embedding_function=embedding_model)\n",
    "\n",
    "def search_vector_db(query, vector_db, k=512):\n",
    "    query = 'search_query: ' + query\n",
    "    most_similar_docs = vector_db.similarity_search_with_relevance_scores(query, k=k)\n",
    "    return most_similar_docs\n",
    "\n",
    "# Peform initial search to load everything into memory\n",
    "search_vector_db(\"Sample query\", langchain_vector_db, k=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67afcd7b-b434-4616-bc39-654e4fd0aa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16809763"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the docs count\n",
    "len(langchain_vector_db.get()['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eeb23e9-004f-4584-a6bf-0fe2c4247264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5c48b2f17b4884a6a837e4e81069dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", config=MistralConfig)\n",
    "model = base_model\n",
    "# model = PeftModel.from_pretrained(base_model, os.path.join('fine_tuning', 'fine_tuned_models'))\n",
    "# model.load_adapter(os.path.join('fine_tuning', 'fine_tuned_models'), 'test_adapter')\n",
    "# model.set_adapter('test_adapter')\n",
    "#model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=16, device=0)\n",
    "pipe.model = model\n",
    "LLM = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac60185-50fc-4cd9-a335-0a982a3f7436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (Jamin Warren: Los Angeles), 1 (John Tarnoff\n",
      "converted_response:\n",
      "['0 (Jamin Warren: Los Angeles)', '1 (John Tarnoff']\n",
      "length:\n",
      "2\n",
      "ID 0:\n",
      "Title: Jamin Warren\n",
      "play, and other seats of culture, from art to music to design.'\" ==Personal life== Warren lives in Los Angeles. ==References== Category:Living people Category:1983 births Category:Harvard College alumni Category:American male journalists Category:American chief executives\n",
      "\n",
      "ID 1:\n",
      "Title: John Tarnoff\n",
      "Santa Monica. He grew up in New York and Paris, and lives in Los Angeles. ==References== Category:1952 births Category:Amherst College alumni Category:Carnegie Mellon University faculty Category:Living people Category:American film producers\n",
      "\n",
      "ID 2:\n",
      "Title: Michiel Vos\n",
      "are always present. They live in Greenwich Village in Manhattan, New York City. Vos wrote a book about his favorite places in New York City. ==References== ==External links== * * My America Category:1970 births Category:Dutch emigrants to the United States Category:Dutch film producers Category:Dutch political journalists Category:Dutch reporters and correspondents Category:Dutch television journalists Category:Living people Category:Pelosi family Category:Writers from Amsterdam\n",
      "\n",
      "ID 3:\n",
      "Title: Douglas Brunt\n",
      "He currently resides in Connecticut with Kelly and their children. https://www.youtube.com/watch?v=TNTzpsWUZaY} == References == == External links == * Category:1971 births Category:Living people Category:American male novelists Category:Writers from Philadelphia Category:Businesspeople from Pennsylvania Category:Duke University alumni Category:American chief executives Category:Haverford School alumni Category:Novelists from Pennsylvania\n",
      "\n",
      "ID 4:\n",
      "Title: Ed Park\n",
      "his M.F.A. from Columbia University. As of 2014, he lives on Manhattan's Upper West Side with his wife and two sons. ==References== ==External links== * Category:Living people Category:1970 births Category:Writers from Buffalo, New York Category:American magazine editors Category:The Village Voice people Category:Los Angeles Times people Category:Yale University alumni Category:Columbia University School of the Arts alumni Category:Novelists from New York (state) Category:People from the Upper West Side\n",
      "\n",
      "ID 5:\n",
      "Title: Michael Levitt (producer)\n",
      "Orange County, California. He resides in Los Angeles, California. ==References== ==External links== * Category:1968 births Category:Living people Category:American television producers Category:People from Roswell, New Mexico\n",
      "\n",
      "ID 6:\n",
      "Title: Daniel Hölzle\n",
      "He resides in Zofingen. == References == Category:Living people Category:1982 births Category:21st-century Swiss politicians Category:Green Party of Switzerland politicians Category:Members of Cantonal Executives of Switzerland Category:Swiss city councillors Category:People from Liestal Category:People from Zofingen\n",
      "\n",
      "ID 7:\n",
      "Title: William Colglazier\n",
      "Aragon High School (CA). He resides in McLean, VA. ==References== Category:21st-century American physicists Category:Living people Category:American chief operating officers Category:1948 births\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"Where does he live??\"\"\"\n",
    "docs_with_score = search_vector_db(user_prompt, langchain_vector_db)\n",
    "\n",
    "keywords = extract_keywords(user_prompt)\n",
    "filtered_docs = contains_keywords_filter(keywords, docs_with_score)\n",
    "filtered_docs = filtered_docs[:8]\n",
    "documents = format_docs_for_LLM(filtered_docs)\n",
    "\n",
    "# prompt_template = \"\"\"<s>[INST] Solve the following prompt. internal_db contains information that may be helpful in solving the prompt.\n",
    "\n",
    "# <internal_db>\n",
    "# {internal_db}\n",
    "# </internal_db>\n",
    "\n",
    "# Prompt: {prompt} [/INST]\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"<s>[INST] Below is a list of documents. Return up to 4 IDs of documents most useful for solving the user_prompt. If no documents are relevant, output -1. {format}. \n",
    "\n",
    "<documents>\n",
    "{documents}\n",
    "</documents>\n",
    "\n",
    "user_prompt: {user_prompt}\n",
    "\n",
    "[/INST]IDs: \"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "chain = prompt | LLM\n",
    "# for doc in filtered_docs:\n",
    "#     print(doc[0].metadata['title'])\n",
    "#     print(doc[0].page_content)\n",
    "#     print('------------')\n",
    "#print(documents)\n",
    "# response = chain.invoke({'internal_db': documents, 'prompt': user_prompt})\n",
    "# print(response)\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "response = chain.invoke({'format':output_parser.get_format_instructions(), 'documents': documents, 'user_prompt': user_prompt})\n",
    "print(response)\n",
    "converted_response = output_parser.parse(response)\n",
    "print('converted_response:')\n",
    "print(converted_response)\n",
    "print('length:')\n",
    "print(len(converted_response))\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff64c09f-51d5-4e61-9f22-429931c39be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0473bcd-3d1f-4adf-9c40-e14518aefeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
